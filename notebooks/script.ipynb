{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e671aa28",
   "metadata": {},
   "source": [
    "## Data Quality com PySpark \n",
    "### 1. Inicializa√ß√£o do Spark e Cria√ß√£o da SparkSession\n",
    "- Este c√≥digo deve ser o primeiro bloco a ser executado no Jupyter Notebook ou Databricks. Ele configura a sess√£o do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o Spark e outras bibliotecas necess√°rias\n",
    "import pyspark # O m√≥dulo principal\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F # O alias 'F' MAI√öSCULO √© usado para todas as fun√ß√µes (F.col, F.when, etc.)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "\n",
    "# Cria ou obt√©m uma SparkSession\n",
    "# Master: 'local[*]' indica que o Spark deve usar todos os n√∫cleos dispon√≠veis na m√°quina\n",
    "# appName: Nome da aplica√ß√£o (importante para monitoramento)\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"AulaPraticaPySpark_DataQuality\")\n",
    "    .config(\"spark.executor.memory\", \"4g\") # Opcional: Configura√ß√µes de mem√≥ria\n",
    "    .config(\"spark.driver.memory\", \"4g\")  # Opcional: Configura√ß√µes de mem√≥ria\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Imprime o status da sess√£o para confirma√ß√£o\n",
    "print(\"SparkSession inicializada com sucesso!\")\n",
    "print(f\"Vers√£o do Spark: {spark.version}\")\n",
    "print(f\"Aplica√ß√£o: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f9476",
   "metadata": {},
   "source": [
    "**Explicando c√≥digo com mais detalhes**\n",
    "- **SparkSession:** √â o principal ponto de entrada para interagir com o cluster Spark. √â como ligar o motor do carro para come√ßar a processar dados distribu√≠dos.\n",
    "\n",
    "- **.master(\"local[*]\"):** Para o ambiente de aula, usamos o modo local. O * diz ao Spark para usar todos os n√∫cleos de CPU dispon√≠veis, simulando um processamento paralelo em um √∫nico n√≥. Em produ√ß√£o, isso seria o endere√ßo do seu gerenciador de cluster (ex: YARN, Mesos ou K8s).\n",
    "\n",
    "- **.config(...):** Permite configurar recursos como mem√≥ria. Em Big Data, √© crucial saber balancear mem√≥ria (executor.memory) e n√∫cleos (num-executors) para otimizar a performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb07f7e7",
   "metadata": {},
   "source": [
    "### 2. Snippet de Carregamento de Dados (Raw Layer)<br>\n",
    "- Para demonstrar as t√©cnicas de Data Quality, usaremos um arquivo chamado vendas_brutas.csv com dados brutos, incluindo nulos e tipos incorretos e definiremos seu schema antes do carregamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f63df5",
   "metadata": {},
   "source": [
    "*DEFINI√á√ÉO DO SCHEMA (CRUCIAL PARA DATA QUALITY E PERFORMANCE)*\n",
    "- **Conex√£o com a Teoria:** Evita o inferSchema, que requer uma leitura completa do arquivo.\n",
    "- Definir o schema antecipadamente garante que os tipos de dados sejam os esperados, prevenindo erros e otimizando a leitura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema_vendas = StructType([\n",
    "    StructField(\"ID_VENDA\", IntegerType(), nullable=False),\n",
    "    StructField(\"DATA_REGISTRO_RAW\", StringType(), nullable=True), # O tipo original √© String, pois o formato √© inconsistente ('2024-03-20' vs '2024/03/20')\n",
    "    StructField(\"VALOR_BRUTO_RAW\", StringType(), nullable=True),  # O tipo original √© String, pois pode vir com v√≠rgula (250,99) ou nulo\n",
    "    StructField(\"PRODUTO\", StringType(), nullable=True),\n",
    "    StructField(\"STATUS_VENDA\", StringType(), nullable=True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef25292",
   "metadata": {},
   "source": [
    "- CARREGAMENTO DO DATAFRAME\n",
    "    - Use um caminho real para o arquivo, ou simule-o usando a API createDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5171961",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_arquivo = \"vendas_brutas.csv\" \n",
    "df_vendas_raw = (\n",
    "    spark.read\n",
    "    .csv(\n",
    "        caminho_arquivo,\n",
    "        header=True,\n",
    "        schema=schema_vendas,\n",
    "        sep=\",\",\n",
    "        # O PySpark l√™ a primeira linha como cabe√ßalho\n",
    "        # e garante que os dados sigam o schema definido (Data Quality!)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da416d03",
   "metadata": {},
   "source": [
    "- INSPE√á√ÉO INICIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e46bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Schema Original (Raw) ---\")\n",
    "df_vendas_raw.printSchema()\n",
    "\n",
    "print(\"\\n--- Primeiros Registros ---\")\n",
    "df_vendas_raw.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d81437",
   "metadata": {},
   "source": [
    "### 3. **Limpeza (Cleaning):** Tratamento de Valores Nulos\n",
    "- **Teoria:** Lidar com missing values (null, None, ou NaN) √© o primeiro passo para garantir a Integridade dos dados. \n",
    "- **Pr√°tica (PySpark):** Usamos .fillna() ou fun√ß√µes condicionais com when/otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802070c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A√ß√£o 1: Limpar/Preencher o campo STATUS_VENDA\n",
    "# Se o status for nulo, vamos preencher com 'EM_PROCESSAMENTO' (regra de neg√≥cio)\n",
    "df_limpeza = df_vendas_raw.withColumn(\n",
    "    \"STATUS_VENDA\",\n",
    "    F.coalesce(F.col(\"STATUS_VENDA\"), F.lit(\"EM_PROCESSAMENTO\")) # Coalesce pega o primeiro valor n√£o nulo\n",
    ")\n",
    "\n",
    "# A√ß√£o 2: Tratamento de nulos em VALOR_BRUTO_RAW\n",
    "# Se o valor for nulo, vamos preencher com 0 (regra de neg√≥cio para vendas n√£o registradas)\n",
    "df_limpeza = df_limpeza.na.fill(value=\"0.0\", subset=['VALOR_BRUTO_RAW'])\n",
    "\n",
    "print(\"\\n--- 1. Limpeza (Cleaning): Nulos Tratados ---\")\n",
    "df_limpeza.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a1bff1",
   "metadata": {},
   "source": [
    "### 4. Transforma√ß√£o (Transformation): Padroniza√ß√£o e Casting de Tipos\n",
    "**Teoria:** Garantir a Consist√™ncia dos dados, convertendo para formatos e tipos padronizados (e.g., String para Date, remo√ß√£o de caracteres inv√°lidos). \n",
    "**Pr√°tica (PySpark):** Uso de fun√ß√µes como regexp_replace, to_date, cast, upper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformacao = (\n",
    "    df_limpeza\n",
    "    # Transforma√ß√£o 1: Padronizar STATUS_VENDA para caixa alta (CONSIST√äNCIA)\n",
    "    .withColumn(\"STATUS_VENDA\", F.upper(F.col(\"STATUS_VENDA\")))\n",
    "\n",
    "    # Transforma√ß√£o 2: Limpar e Converter VALOR_BRUTO_RAW para Decimal (TIPAGEM CORRETA)\n",
    "    # 1. Substitui v√≠rgulas (',') por ponto ('.')\n",
    "    # 2. Converte o resultado para tipo Double\n",
    "    .withColumn(\n",
    "        \"VALOR_VENDA\",\n",
    "        F.regexp_replace(F.col(\"VALOR_BRUTO_RAW\"), \",\", \".\").cast(DoubleType())\n",
    "    )\n",
    "\n",
    "    # Transforma√ß√£o 3: Converter DATA_REGISTRO_RAW para Tipo Date (FORMATO CORRETO)\n",
    "    # Tenta inferir o formato de data (yyyy-MM-dd, yyyy/MM/dd, etc.) - PySpark √© flex√≠vel\n",
    "    .withColumn(\n",
    "        \"DATA_VENDA\",\n",
    "        F.to_date(F.col(\"DATA_REGISTRO_RAW\"), \"yyyy-MM-dd\") # Tenta formato padr√£o, o PySpark pode inferir varia√ß√µes simples\n",
    "    )\n",
    "    .withColumn(\"DATA_VENDA\", F.coalesce(F.col(\"DATA_VENDA\"), F.to_date(F.col(\"DATA_REGISTRO_RAW\"), \"yyyy/MM/dd\"))) # Tenta o formato com barra, caso o anterior falhe\n",
    "    .drop(\"DATA_REGISTRO_RAW\", \"VALOR_BRUTO_RAW\") # Remove as colunas RAW\n",
    ")\n",
    "\n",
    "print(\"\\n--- 2. Transforma√ß√£o: Padroniza√ß√£o e Casting ---\")\n",
    "df_transformacao.printSchema()\n",
    "df_transformacao.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae8b7c",
   "metadata": {},
   "source": [
    "### 5. Enriquecimento (Enrichment): Adi√ß√£o de Colunas de Neg√≥cio\n",
    "**Teoria:** Adicionar valor ao dataset derivando novos campos de dados existentes, criando features √∫teis para an√°lise.\n",
    "\n",
    "**Pr√°tica (PySpark):** Uso de withColumn com express√µes l√≥gicas (when/otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A√ß√£o: Criar uma nova coluna indicando se a venda √© considerada de \"Alto Valor\" (> 200)\n",
    "df_enriquecimento = df_transformacao.withColumn(\n",
    "    \"FLAG_ALTO_VALOR\",\n",
    "    F.when(F.col(\"VALOR_VENDA\") >= 200, F.lit(True)).otherwise(F.lit(False))\n",
    ")\n",
    "\n",
    "print(\"\\n--- 3. Enriquecimento: Nova Feature Adicionada ---\")\n",
    "df_enriquecimento.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde3ae7",
   "metadata": {},
   "source": [
    "### 6. Normaliza√ß√£o (Normalization): Padroniza√ß√£o de Categorias\n",
    "**Teoria:** Reduzir a vari√¢ncia de valores categ√≥ricos para garantir que a mesma entidade tenha sempre a mesma representa√ß√£o.\n",
    "\n",
    "**Pr√°tica (PySpark):** Aplicar lookups ou regras de substitui√ß√£o em campos como PRODUTO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a332924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A√ß√£o: Padronizar o nome do produto (ex: 'LAPTOP' pode vir como 'NoteBook', 'Laptop')\n",
    "# Vamos usar o PRODUTO para garantir que todos sejam capitalizados\n",
    "df_normalizacao = df_enriquecimento.withColumn(\n",
    "    \"PRODUTO_PADRAO\",\n",
    "    F.when(F.upper(F.col(\"PRODUTO\")).like(\"%LAPTOP%\"), F.lit(\"NOTEBOOK/LAPTOP\"))\n",
    "    .when(F.upper(F.col(\"PRODUTO\")).like(\"%MOUSE%\"), F.lit(\"PERIFERICO_SIMPLES\"))\n",
    "    .otherwise(F.upper(F.col(\"PRODUTO\")))\n",
    ").drop(\"PRODUTO\").withColumnRenamed(\"PRODUTO_PADRAO\", \"PRODUTO\") # Substitui a coluna original\n",
    "\n",
    "print(\"\\n--- 4. Normaliza√ß√£o: Categorias Padronizadas ---\")\n",
    "df_normalizacao.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a09356",
   "metadata": {},
   "source": [
    "### 7. Desduplica√ß√£o (Deduplication): Remo√ß√£o de Registros Redundantes\n",
    "**Teoria:** Garantir que cada evento/entidade √∫nica seja representada apenas uma vez no dataset final (fundamental para Precis√£o).\n",
    "\n",
    "**Pr√°tica (PySpark):** Usar .dropDuplicates() ou Window Functions para cen√°rios avan√ßados (como manter o registro mais recente).\n",
    "No nosso exemplo, as linhas 1004 e 1005 s√£o duplicatas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40466028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos apenas as colunas que definem a unicidade do registro (chave prim√°ria natural)\n",
    "colunas_chave = [\"ID_VENDA\"]\n",
    "\n",
    "# A√ß√£o: Remover duplicatas estritas\n",
    "df_curated = df_normalizacao.dropDuplicates(subset=colunas_chave)\n",
    "\n",
    "# Demonstra√ß√£o de Desduplica√ß√£o Avan√ßada (Teoria: Manter o Mais Recente)\n",
    "# Isso √© √∫til quando ID_VENDA pudesse se repetir com vers√µes diferentes.\n",
    "# w = Window.partitionBy(\"ID_VENDA\").orderBy(F.col(\"DATA_VENDA\").desc())\n",
    "# df_curated_avancado = (df_normalizacao\n",
    "#     .withColumn(\"row_number\", F.row_number().over(w))\n",
    "#     .filter(F.col(\"row_number\") == 1)\n",
    "#     .drop(\"row_number\")\n",
    "# )\n",
    "\n",
    "print(f\"\\n--- 5. Desduplica√ß√£o: Registros √önicos ({df_normalizacao.count()} vs {df_curated.count()}) ---\")\n",
    "df_curated.show(truncate=False)\n",
    "df_curated.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f529d2",
   "metadata": {},
   "source": [
    "### 8. Armazenamento Otimizado (Parquet & ORC)\n",
    "1. Escrita no Formato Parquet (Otimiza√ß√£o Colunar)\n",
    "- **Conex√£o com a Teoria:**\n",
    "Parquet: √â o formato padr√£o da ind√∫stria, otimizado para consultas anal√≠ticas (OLAP). O armazenamento colunar permite que o Spark (e o SerDe do Parquet) leia apenas as colunas que a query solicita, ignorando o resto.\n",
    "\n",
    "- **Particionamento:** Usar partitionBy() organiza os dados fisicamente no disco/storage (e.g., por ano/m√™s ou por STATUS_VENDA). Isso permite ao Spark ignorar diret√≥rios inteiros (Predicate Pushdown).\n",
    "\n",
    "- **Compacta√ß√£o:** Usaremos a compacta√ß√£o Snappy (o padr√£o para Parquet), que oferece um bom equil√≠brio entre taxa de compress√£o e velocidade de descompacta√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ad4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o caminho de sa√≠da (simula√ß√£o de um Data Lake)\n",
    "caminho_parquet = \"data/curated/vendas_parquet\"\n",
    "\n",
    "# A√ß√£o: Salvar o DataFrame tratado (df_curated) em Parquet\n",
    "(\n",
    "    df_curated.write\n",
    "    .mode(\"overwrite\") # Sobrescreve se o diret√≥rio j√° existir\n",
    "    .partitionBy(\"STATUS_VENDA\") # Particionamento f√≠sico: otimiza queries que filtram por status\n",
    "    .option(\"compression\", \"snappy\") # Snappy √© o padr√£o (bom trade-off)\n",
    "    .parquet(caminho_parquet)\n",
    ")\n",
    "\n",
    "print(f\"\\n--- 1. Escrita em Parquet conclu√≠da ---\")\n",
    "print(f\"Dados salvos e particionados em: {caminho_parquet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df74669",
   "metadata": {},
   "source": [
    "2. Escrita no Formato ORC (Alternativa Colunar)\n",
    "üíæ Conex√£o com a Teoria:\n",
    "ORC (Optimized Row Columnar): Outro formato colunar robusto, popular no ecossistema Hive. Possui recursos avan√ßados como indexa√ß√£o de dados e Bloom Filters, que podem ser mais eficientes em certos cen√°rios de leitura seletiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o caminho de sa√≠da\n",
    "caminho_orc = \"data/curated/vendas_orc\"\n",
    "\n",
    "# A√ß√£o: Salvar o mesmo DataFrame em ORC\n",
    "(\n",
    "    df_curated.write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"STATUS_VENDA\")\n",
    "    .orc(caminho_orc)\n",
    ")\n",
    "\n",
    "print(f\"\\n--- 2. Escrita em ORC conclu√≠da ---\")\n",
    "print(f\"Dados salvos e particionados em: {caminho_orc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccfa330",
   "metadata": {},
   "source": [
    "### 9.An√°lise da Otimiza√ß√£o (O Ponto Alto da Aula)\n",
    "- Agora vamos provar a teoria do Predicate Pushdown e do SerDe na pr√°tica.\n",
    "\n",
    "1. Pr√°tica do Predicate Pushdown (Ignorando Arquivos)\n",
    "**Teoria:** Ao particionarmos por STATUS_VENDA, se pedirmos apenas vendas 'CANCELADO', o Spark ignora os diret√≥rios 'CONCLUIDO' e 'EM_PROCESSAMENTO'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ff8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura Otimizada - Filtro na parti√ß√£o\n",
    "# Apenas os diret√≥rios (parti√ß√µes) que cont√™m 'CANCELADO' ser√£o lidos\n",
    "df_leitura_filtrada = spark.read.parquet(caminho_parquet).filter(F.col(\"STATUS_VENDA\") == \"CANCELADO\")\n",
    "\n",
    "print(\"\\n--- 3.1. Predicate Pushdown: Filtro no Disco ---\")\n",
    "print(f\"Total de Registros lidos: {df_leitura_filtrada.count()}\")\n",
    "df_leitura_filtrada.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03dfd6",
   "metadata": {},
   "source": [
    "2. Pr√°tica do SerDe (Deserializando Apenas o Necess√°rio)\n",
    "Teoria: O SerDe (Serializer/Deserializer) do Parquet s√≥ desserializa as colunas que voc√™ projeta na sua query. Se lermos apenas ID_VENDA, o Spark n√£o carrega os bytes das colunas VALOR_VENDA ou PRODUTO para a mem√≥ria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afdf68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura Otimizada - Sele√ß√£o de Colunas (Proje√ß√£o)\n",
    "# O Spark carrega apenas os bytes da coluna 'ID_VENDA' e 'VALOR_VENDA'\n",
    "df_projecao = spark.read.parquet(caminho_parquet).select(\"ID_VENDA\", \"VALOR_VENDA\")\n",
    "\n",
    "print(\"\\n--- 3.2. SerDe: Deserializa√ß√£o Seletiva (Proje√ß√£o) ---\")\n",
    "print(f\"Colunas carregadas: {df_projecao.columns}\")\n",
    "df_projecao.printSchema()\n",
    "\n",
    "# Ponto de Discuss√£o:\n",
    "# Compara√ß√£o 1: Se este fosse um arquivo CSV, o SerDe leria a linha inteira para depois descartar as colunas n√£o pedidas.\n",
    "# Compara√ß√£o 2: No Parquet, o Spark/SerDe acessa apenas a 'faixa' vertical da coluna no arquivo f√≠sico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e015a",
   "metadata": {},
   "source": [
    "# üöÄ Conclus√£o: PySpark - Da Teoria √† Produ√ß√£o\n",
    "\n",
    "Parab√©ns! \n",
    "\n",
    "Voc√™s completaram com sucesso a jornada pr√°tica de 4 horas, aplicando os fundamentos de Big Data em um pipeline PySpark funcional.\n",
    "\n",
    "## üìù Resumo do Aprendizado Essencial\n",
    "\n",
    "Onde a Teoria e a Pr√°tica se Encontraram:\n",
    "\n",
    "| Teoria (Aulas) | Pr√°tica (C√≥digo PySpark) | Conceito Refor√ßado |\n",
    "| :--- | :--- | :--- |\n",
    "| **Data Pipeline: Processing** | Inicializa√ß√£o da `SparkSession` | PySpark √© o motor de **refino** do dado bruto (`Raw Layer`). |\n",
    "| **Data Quality (5 T√©cnicas)** | `F.coalesce()`, `to_date()`, `dropDuplicates()` | A disciplina de **qualidade** (Limpeza, Transforma√ß√£o, Desduplica√ß√£o) √© implementada com fun√ß√µes *built-in* do Spark. |\n",
    "| **Formatos Colunares** | `.write.parquet()`, `.write.orc()` | **Parquet** e **ORC** s√£o os formatos de armazenamento otimizado para economia de I/O e *analytics*. |\n",
    "| **SerDe & Otimiza√ß√£o** | `.read.parquet().select(\"col\")` | O Spark/SerDe acessa apenas as colunas necess√°rias (**Proje√ß√£o**) e ignora diret√≥rios (**Predicate Pushdown**). |\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Pr√≥ximos Passos e Conex√£o com o Projeto Final\n",
    "\n",
    "Para que voc√™s transformem este conhecimento pr√°tico em uma solu√ß√£o de Engenharia de Dados de n√≠vel profissional, o foco deve ser na moderniza√ß√£o e automa√ß√£o da arquitetura:\n",
    "\n",
    "### 1. Data Lakehouse: Confiabilidade sobre o Storage\n",
    "\n",
    "O `df_curated` salvo em Parquet √© a nossa base. O pr√≥ximo passo √© garantir **Transa√ß√µes ACID** e **versionamento** sobre ele.\n",
    "\n",
    "* **Pr√≥ximo T√≥pico:** Formatos de Tabela Aberta como **Delta Lake** (ou Apache Iceberg).\n",
    "* **Motivo:** Permitem funcionalidades cruciais de um Data Warehouse, como `UPSERT` (atualiza√ß√£o/inser√ß√£o) e `DELETE` direto no Data Lake.\n",
    "\n",
    "### 2. Orquestra√ß√£o e Automa√ß√£o do Pipeline\n",
    "\n",
    "Um *pipeline* de dados precisa ser executado de forma autom√°tica e monitorada.\n",
    "\n",
    "* **Pr√≥ximo T√≥pico:** **Apache Airflow**.\n",
    "* **Objetivo:** Agendar o c√≥digo PySpark desenvolvido hoje (as transforma√ß√µes e a escrita em Parquet) em um *workflow* (DAG) gerenciado, garantindo automa√ß√£o e observabilidade.\n",
    "\n",
    "### 3. Prepara√ß√£o para o Projeto Final\n",
    "\n",
    "Utilizem o `df_curated` e a l√≥gica de Data Quality implementada hoje como o *core* da fase **Processing** do seu projeto. A pr√≥xima etapa √© integrar o Delta Lake e construir a automa√ß√£o no Airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982402dd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
