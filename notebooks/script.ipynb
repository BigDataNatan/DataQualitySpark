{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e671aa28",
   "metadata": {},
   "source": [
    "## Data Quality com PySpark \n",
    "### 1. Inicialização do Spark e Criação da SparkSession\n",
    "- Este código deve ser o primeiro bloco a ser executado no Jupyter Notebook ou Databricks. Ele configura a sessão do Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o Spark e outras bibliotecas necessárias\n",
    "import pyspark # O módulo principal\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F # O alias 'F' MAIÚSCULO é usado para todas as funções (F.col, F.when, etc.)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "\n",
    "# Cria ou obtém uma SparkSession\n",
    "# Master: 'local[*]' indica que o Spark deve usar todos os núcleos disponíveis na máquina\n",
    "# appName: Nome da aplicação (importante para monitoramento)\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"AulaPraticaPySpark_DataQuality\")\n",
    "    .config(\"spark.executor.memory\", \"4g\") # Opcional: Configurações de memória\n",
    "    .config(\"spark.driver.memory\", \"4g\")  # Opcional: Configurações de memória\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Imprime o status da sessão para confirmação\n",
    "print(\"SparkSession inicializada com sucesso!\")\n",
    "print(f\"Versão do Spark: {spark.version}\")\n",
    "print(f\"Aplicação: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f9476",
   "metadata": {},
   "source": [
    "**Explicando código com mais detalhes**\n",
    "- **SparkSession:** É o principal ponto de entrada para interagir com o cluster Spark. É como ligar o motor do carro para começar a processar dados distribuídos.\n",
    "\n",
    "- **.master(\"local[*]\"):** Para o ambiente de aula, usamos o modo local. O * diz ao Spark para usar todos os núcleos de CPU disponíveis, simulando um processamento paralelo em um único nó. Em produção, isso seria o endereço do seu gerenciador de cluster (ex: YARN, Mesos ou K8s).\n",
    "\n",
    "- **.config(...):** Permite configurar recursos como memória. Em Big Data, é crucial saber balancear memória (executor.memory) e núcleos (num-executors) para otimizar a performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb07f7e7",
   "metadata": {},
   "source": [
    "### 2. Snippet de Carregamento de Dados (Raw Layer)<br>\n",
    "- Para demonstrar as técnicas de Data Quality, usaremos um arquivo chamado vendas_brutas.csv com dados brutos, incluindo nulos e tipos incorretos e definiremos seu schema antes do carregamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f63df5",
   "metadata": {},
   "source": [
    "*DEFINIÇÃO DO SCHEMA (CRUCIAL PARA DATA QUALITY E PERFORMANCE)*\n",
    "- **Conexão com a Teoria:** Evita o inferSchema, que requer uma leitura completa do arquivo.\n",
    "- Definir o schema antecipadamente garante que os tipos de dados sejam os esperados, prevenindo erros e otimizando a leitura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema_vendas = StructType([\n",
    "    StructField(\"ID_VENDA\", IntegerType(), nullable=False),\n",
    "    StructField(\"DATA_REGISTRO_RAW\", StringType(), nullable=True), # O tipo original é String, pois o formato é inconsistente ('2024-03-20' vs '2024/03/20')\n",
    "    StructField(\"VALOR_BRUTO_RAW\", StringType(), nullable=True),  # O tipo original é String, pois pode vir com vírgula (250,99) ou nulo\n",
    "    StructField(\"PRODUTO\", StringType(), nullable=True),\n",
    "    StructField(\"STATUS_VENDA\", StringType(), nullable=True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef25292",
   "metadata": {},
   "source": [
    "- CARREGAMENTO DO DATAFRAME\n",
    "    - Use um caminho real para o arquivo, ou simule-o usando a API createDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5171961",
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_arquivo = \"vendas_brutas.csv\" \n",
    "df_vendas_raw = (\n",
    "    spark.read\n",
    "    .csv(\n",
    "        caminho_arquivo,\n",
    "        header=True,\n",
    "        schema=schema_vendas,\n",
    "        sep=\",\",\n",
    "        # O PySpark lê a primeira linha como cabeçalho\n",
    "        # e garante que os dados sigam o schema definido (Data Quality!)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da416d03",
   "metadata": {},
   "source": [
    "- INSPEÇÃO INICIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e46bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Schema Original (Raw) ---\")\n",
    "df_vendas_raw.printSchema()\n",
    "\n",
    "print(\"\\n--- Primeiros Registros ---\")\n",
    "df_vendas_raw.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d81437",
   "metadata": {},
   "source": [
    "### 3. **Limpeza (Cleaning):** Tratamento de Valores Nulos\n",
    "- **Teoria:** Lidar com missing values (null, None, ou NaN) é o primeiro passo para garantir a Integridade dos dados. \n",
    "- **Prática (PySpark):** Usamos .fillna() ou funções condicionais com when/otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802070c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ação 1: Limpar/Preencher o campo STATUS_VENDA\n",
    "# Se o status for nulo, vamos preencher com 'EM_PROCESSAMENTO' (regra de negócio)\n",
    "df_limpeza = df_vendas_raw.withColumn(\n",
    "    \"STATUS_VENDA\",\n",
    "    F.coalesce(F.col(\"STATUS_VENDA\"), F.lit(\"EM_PROCESSAMENTO\")) # Coalesce pega o primeiro valor não nulo\n",
    ")\n",
    "\n",
    "# Ação 2: Tratamento de nulos em VALOR_BRUTO_RAW\n",
    "# Se o valor for nulo, vamos preencher com 0 (regra de negócio para vendas não registradas)\n",
    "df_limpeza = df_limpeza.na.fill(value=\"0.0\", subset=['VALOR_BRUTO_RAW'])\n",
    "\n",
    "print(\"\\n--- 1. Limpeza (Cleaning): Nulos Tratados ---\")\n",
    "df_limpeza.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a1bff1",
   "metadata": {},
   "source": [
    "### 4. Transformação (Transformation): Padronização e Casting de Tipos\n",
    "**Teoria:** Garantir a Consistência dos dados, convertendo para formatos e tipos padronizados (e.g., String para Date, remoção de caracteres inválidos). \n",
    "**Prática (PySpark):** Uso de funções como regexp_replace, to_date, cast, upper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformacao = (\n",
    "    df_limpeza\n",
    "    # Transformação 1: Padronizar STATUS_VENDA para caixa alta (CONSISTÊNCIA)\n",
    "    .withColumn(\"STATUS_VENDA\", F.upper(F.col(\"STATUS_VENDA\")))\n",
    "\n",
    "    # Transformação 2: Limpar e Converter VALOR_BRUTO_RAW para Decimal (TIPAGEM CORRETA)\n",
    "    # 1. Substitui vírgulas (',') por ponto ('.')\n",
    "    # 2. Converte o resultado para tipo Double\n",
    "    .withColumn(\n",
    "        \"VALOR_VENDA\",\n",
    "        F.regexp_replace(F.col(\"VALOR_BRUTO_RAW\"), \",\", \".\").cast(DoubleType())\n",
    "    )\n",
    "\n",
    "    # Transformação 3: Converter DATA_REGISTRO_RAW para Tipo Date (FORMATO CORRETO)\n",
    "    # Tenta inferir o formato de data (yyyy-MM-dd, yyyy/MM/dd, etc.) - PySpark é flexível\n",
    "    .withColumn(\n",
    "        \"DATA_VENDA\",\n",
    "        F.to_date(F.col(\"DATA_REGISTRO_RAW\"), \"yyyy-MM-dd\") # Tenta formato padrão, o PySpark pode inferir variações simples\n",
    "    )\n",
    "    .withColumn(\"DATA_VENDA\", F.coalesce(F.col(\"DATA_VENDA\"), F.to_date(F.col(\"DATA_REGISTRO_RAW\"), \"yyyy/MM/dd\"))) # Tenta o formato com barra, caso o anterior falhe\n",
    "    .drop(\"DATA_REGISTRO_RAW\", \"VALOR_BRUTO_RAW\") # Remove as colunas RAW\n",
    ")\n",
    "\n",
    "print(\"\\n--- 2. Transformação: Padronização e Casting ---\")\n",
    "df_transformacao.printSchema()\n",
    "df_transformacao.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae8b7c",
   "metadata": {},
   "source": [
    "### 5. Enriquecimento (Enrichment): Adição de Colunas de Negócio\n",
    "**Teoria:** Adicionar valor ao dataset derivando novos campos de dados existentes, criando features úteis para análise.\n",
    "\n",
    "**Prática (PySpark):** Uso de withColumn com expressões lógicas (when/otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ação: Criar uma nova coluna indicando se a venda é considerada de \"Alto Valor\" (> 200)\n",
    "df_enriquecimento = df_transformacao.withColumn(\n",
    "    \"FLAG_ALTO_VALOR\",\n",
    "    F.when(F.col(\"VALOR_VENDA\") >= 200, F.lit(True)).otherwise(F.lit(False))\n",
    ")\n",
    "\n",
    "print(\"\\n--- 3. Enriquecimento: Nova Feature Adicionada ---\")\n",
    "df_enriquecimento.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde3ae7",
   "metadata": {},
   "source": [
    "### 6. Normalização (Normalization): Padronização de Categorias\n",
    "**Teoria:** Reduzir a variância de valores categóricos para garantir que a mesma entidade tenha sempre a mesma representação.\n",
    "\n",
    "**Prática (PySpark):** Aplicar lookups ou regras de substituição em campos como PRODUTO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a332924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ação: Padronizar o nome do produto (ex: 'LAPTOP' pode vir como 'NoteBook', 'Laptop')\n",
    "# Vamos usar o PRODUTO para garantir que todos sejam capitalizados\n",
    "df_normalizacao = df_enriquecimento.withColumn(\n",
    "    \"PRODUTO_PADRAO\",\n",
    "    F.when(F.upper(F.col(\"PRODUTO\")).like(\"%LAPTOP%\"), F.lit(\"NOTEBOOK/LAPTOP\"))\n",
    "    .when(F.upper(F.col(\"PRODUTO\")).like(\"%MOUSE%\"), F.lit(\"PERIFERICO_SIMPLES\"))\n",
    "    .otherwise(F.upper(F.col(\"PRODUTO\")))\n",
    ").drop(\"PRODUTO\").withColumnRenamed(\"PRODUTO_PADRAO\", \"PRODUTO\") # Substitui a coluna original\n",
    "\n",
    "print(\"\\n--- 4. Normalização: Categorias Padronizadas ---\")\n",
    "df_normalizacao.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a09356",
   "metadata": {},
   "source": [
    "### 7. Desduplicação (Deduplication): Remoção de Registros Redundantes\n",
    "**Teoria:** Garantir que cada evento/entidade única seja representada apenas uma vez no dataset final (fundamental para Precisão).\n",
    "\n",
    "**Prática (PySpark):** Usar .dropDuplicates() ou Window Functions para cenários avançados (como manter o registro mais recente).\n",
    "No nosso exemplo, as linhas 1004 e 1005 são duplicatas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40466028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos apenas as colunas que definem a unicidade do registro (chave primária natural)\n",
    "colunas_chave = [\"ID_VENDA\"]\n",
    "\n",
    "# Ação: Remover duplicatas estritas\n",
    "df_curated = df_normalizacao.dropDuplicates(subset=colunas_chave)\n",
    "\n",
    "# Demonstração de Desduplicação Avançada (Teoria: Manter o Mais Recente)\n",
    "# Isso é útil quando ID_VENDA pudesse se repetir com versões diferentes.\n",
    "# w = Window.partitionBy(\"ID_VENDA\").orderBy(F.col(\"DATA_VENDA\").desc())\n",
    "# df_curated_avancado = (df_normalizacao\n",
    "#     .withColumn(\"row_number\", F.row_number().over(w))\n",
    "#     .filter(F.col(\"row_number\") == 1)\n",
    "#     .drop(\"row_number\")\n",
    "# )\n",
    "\n",
    "print(f\"\\n--- 5. Desduplicação: Registros Únicos ({df_normalizacao.count()} vs {df_curated.count()}) ---\")\n",
    "df_curated.show(truncate=False)\n",
    "df_curated.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f529d2",
   "metadata": {},
   "source": [
    "### 8. Armazenamento Otimizado (Parquet & ORC)\n",
    "1. Escrita no Formato Parquet (Otimização Colunar)\n",
    "- **Conexão com a Teoria:**\n",
    "Parquet: É o formato padrão da indústria, otimizado para consultas analíticas (OLAP). O armazenamento colunar permite que o Spark (e o SerDe do Parquet) leia apenas as colunas que a query solicita, ignorando o resto.\n",
    "\n",
    "- **Particionamento:** Usar partitionBy() organiza os dados fisicamente no disco/storage (e.g., por ano/mês ou por STATUS_VENDA). Isso permite ao Spark ignorar diretórios inteiros (Predicate Pushdown).\n",
    "\n",
    "- **Compactação:** Usaremos a compactação Snappy (o padrão para Parquet), que oferece um bom equilíbrio entre taxa de compressão e velocidade de descompactação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ad4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o caminho de saída (simulação de um Data Lake)\n",
    "caminho_parquet = \"data/curated/vendas_parquet\"\n",
    "\n",
    "# Ação: Salvar o DataFrame tratado (df_curated) em Parquet\n",
    "(\n",
    "    df_curated.write\n",
    "    .mode(\"overwrite\") # Sobrescreve se o diretório já existir\n",
    "    .partitionBy(\"STATUS_VENDA\") # Particionamento físico: otimiza queries que filtram por status\n",
    "    .option(\"compression\", \"snappy\") # Snappy é o padrão (bom trade-off)\n",
    "    .parquet(caminho_parquet)\n",
    ")\n",
    "\n",
    "print(f\"\\n--- 1. Escrita em Parquet concluída ---\")\n",
    "print(f\"Dados salvos e particionados em: {caminho_parquet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df74669",
   "metadata": {},
   "source": [
    "2. Escrita no Formato ORC (Alternativa Colunar)\n",
    "💾 Conexão com a Teoria:\n",
    "ORC (Optimized Row Columnar): Outro formato colunar robusto, popular no ecossistema Hive. Possui recursos avançados como indexação de dados e Bloom Filters, que podem ser mais eficientes em certos cenários de leitura seletiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o caminho de saída\n",
    "caminho_orc = \"data/curated/vendas_orc\"\n",
    "\n",
    "# Ação: Salvar o mesmo DataFrame em ORC\n",
    "(\n",
    "    df_curated.write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"STATUS_VENDA\")\n",
    "    .orc(caminho_orc)\n",
    ")\n",
    "\n",
    "print(f\"\\n--- 2. Escrita em ORC concluída ---\")\n",
    "print(f\"Dados salvos e particionados em: {caminho_orc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccfa330",
   "metadata": {},
   "source": [
    "### 9.Análise da Otimização (O Ponto Alto da Aula)\n",
    "- Agora vamos provar a teoria do Predicate Pushdown e do SerDe na prática.\n",
    "\n",
    "1. Prática do Predicate Pushdown (Ignorando Arquivos)\n",
    "**Teoria:** Ao particionarmos por STATUS_VENDA, se pedirmos apenas vendas 'CANCELADO', o Spark ignora os diretórios 'CONCLUIDO' e 'EM_PROCESSAMENTO'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ff8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura Otimizada - Filtro na partição\n",
    "# Apenas os diretórios (partições) que contêm 'CANCELADO' serão lidos\n",
    "df_leitura_filtrada = spark.read.parquet(caminho_parquet).filter(F.col(\"STATUS_VENDA\") == \"CANCELADO\")\n",
    "\n",
    "print(\"\\n--- 3.1. Predicate Pushdown: Filtro no Disco ---\")\n",
    "print(f\"Total de Registros lidos: {df_leitura_filtrada.count()}\")\n",
    "df_leitura_filtrada.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03dfd6",
   "metadata": {},
   "source": [
    "2. Prática do SerDe (Deserializando Apenas o Necessário)\n",
    "Teoria: O SerDe (Serializer/Deserializer) do Parquet só desserializa as colunas que você projeta na sua query. Se lermos apenas ID_VENDA, o Spark não carrega os bytes das colunas VALOR_VENDA ou PRODUTO para a memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afdf68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura Otimizada - Seleção de Colunas (Projeção)\n",
    "# O Spark carrega apenas os bytes da coluna 'ID_VENDA' e 'VALOR_VENDA'\n",
    "df_projecao = spark.read.parquet(caminho_parquet).select(\"ID_VENDA\", \"VALOR_VENDA\")\n",
    "\n",
    "print(\"\\n--- 3.2. SerDe: Deserialização Seletiva (Projeção) ---\")\n",
    "print(f\"Colunas carregadas: {df_projecao.columns}\")\n",
    "df_projecao.printSchema()\n",
    "\n",
    "# Ponto de Discussão:\n",
    "# Comparação 1: Se este fosse um arquivo CSV, o SerDe leria a linha inteira para depois descartar as colunas não pedidas.\n",
    "# Comparação 2: No Parquet, o Spark/SerDe acessa apenas a 'faixa' vertical da coluna no arquivo físico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e015a",
   "metadata": {},
   "source": [
    "# 🚀 Conclusão: PySpark - Da Teoria à Produção\n",
    "\n",
    "Parabéns! \n",
    "\n",
    "Vocês completaram com sucesso a jornada prática de 4 horas, aplicando os fundamentos de Big Data em um pipeline PySpark funcional.\n",
    "\n",
    "## 📝 Resumo do Aprendizado Essencial\n",
    "\n",
    "Onde a Teoria e a Prática se Encontraram:\n",
    "\n",
    "| Teoria (Aulas) | Prática (Código PySpark) | Conceito Reforçado |\n",
    "| :--- | :--- | :--- |\n",
    "| **Data Pipeline: Processing** | Inicialização da `SparkSession` | PySpark é o motor de **refino** do dado bruto (`Raw Layer`). |\n",
    "| **Data Quality (5 Técnicas)** | `F.coalesce()`, `to_date()`, `dropDuplicates()` | A disciplina de **qualidade** (Limpeza, Transformação, Desduplicação) é implementada com funções *built-in* do Spark. |\n",
    "| **Formatos Colunares** | `.write.parquet()`, `.write.orc()` | **Parquet** e **ORC** são os formatos de armazenamento otimizado para economia de I/O e *analytics*. |\n",
    "| **SerDe & Otimização** | `.read.parquet().select(\"col\")` | O Spark/SerDe acessa apenas as colunas necessárias (**Projeção**) e ignora diretórios (**Predicate Pushdown**). |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 Próximos Passos e Conexão com o Projeto Final\n",
    "\n",
    "Para que vocês transformem este conhecimento prático em uma solução de Engenharia de Dados de nível profissional, o foco deve ser na modernização e automação da arquitetura:\n",
    "\n",
    "### 1. Data Lakehouse: Confiabilidade sobre o Storage\n",
    "\n",
    "O `df_curated` salvo em Parquet é a nossa base. O próximo passo é garantir **Transações ACID** e **versionamento** sobre ele.\n",
    "\n",
    "* **Próximo Tópico:** Formatos de Tabela Aberta como **Delta Lake** (ou Apache Iceberg).\n",
    "* **Motivo:** Permitem funcionalidades cruciais de um Data Warehouse, como `UPSERT` (atualização/inserção) e `DELETE` direto no Data Lake.\n",
    "\n",
    "### 2. Orquestração e Automação do Pipeline\n",
    "\n",
    "Um *pipeline* de dados precisa ser executado de forma automática e monitorada.\n",
    "\n",
    "* **Próximo Tópico:** **Apache Airflow**.\n",
    "* **Objetivo:** Agendar o código PySpark desenvolvido hoje (as transformações e a escrita em Parquet) em um *workflow* (DAG) gerenciado, garantindo automação e observabilidade.\n",
    "\n",
    "### 3. Preparação para o Projeto Final\n",
    "\n",
    "Utilizem o `df_curated` e a lógica de Data Quality implementada hoje como o *core* da fase **Processing** do seu projeto. A próxima etapa é integrar o Delta Lake e construir a automação no Airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982402dd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
