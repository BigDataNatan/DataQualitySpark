# Projeto de Data Quality com PySpark

Este reposit√≥rio foi criado com o objetivo de fornecer um guia pr√°tico e introdut√≥rio ao Apache Spark, utilizando a linguagem Python (PySpark). O foco principal √© demonstrar um fluxo de trabalho essencial de engenharia de dados, cobrindo desde a ingest√£o de dados brutos at√© a aplica√ß√£o de t√©cnicas de qualidade de dados e o armazenamento em formatos otimizados.

## üöÄ Conceitos Abordados

O projeto, contido no notebook `notebooks/script.ipynb`, aborda os seguintes pilares de processamento de dados com Spark:

1.  **Inicializa√ß√£o e Configura√ß√£o**:
    *   Cria√ß√£o de uma `SparkSession`, o ponto de entrada para qualquer aplica√ß√£o Spark.

2.  **Ingest√£o de Dados**:
    *   Leitura de arquivos CSV, definindo um **schema expl√≠cito** para garantir a consist√™ncia dos tipos de dados e otimizar a performance de leitura.

3.  **Pipeline de Qualidade de Dados (Data Quality)**:
    *   **Limpeza (Cleaning)**: Tratamento de valores nulos (`null`) utilizando `coalesce` e `fillna`.
    *   **Transforma√ß√£o (Transformation)**: Padroniza√ß√£o de formatos (datas, strings) e convers√£o de tipos de dados (`cast`).
    *   **Enriquecimento (Enrichment)**: Cria√ß√£o de novas colunas com base em regras de neg√≥cio (`when`/`otherwise`).
    *   **Normaliza√ß√£o (Normalization)**: Padroniza√ß√£o de valores categ√≥ricos para garantir consist√™ncia.
    *   **Desduplica√ß√£o (Deduplication)**: Remo√ß√£o de registros duplicados para garantir a unicidade dos dados.

4.  **Armazenamento Otimizado**:
    *   Escrita dos dados tratados em formato **Parquet** e **ORC**, explicando os benef√≠cios de formatos colunares.
    *   Uso de **particionamento** (`partitionBy`) para otimizar futuras consultas.

5.  **Otimiza√ß√£o de Consultas**:
    *   Demonstra√ß√£o pr√°tica do **Predicate Pushdown**, onde o Spark otimiza a leitura ao filtrar dados diretamente no n√≠vel das parti√ß√µes.
    *   Demonstra√ß√£o da otimiza√ß√£o de **SerDe (Projection Pushdown)**, onde apenas as colunas selecionadas em uma query s√£o lidas do disco.

## üìÇ Estrutura do Projeto

```
/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ logistica_raw.csv       # Dataset bruto utilizado no exemplo.
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ script.ipynb            # Notebook Jupyter com todo o c√≥digo PySpark.
‚îú‚îÄ‚îÄ docker-compose.yml          # Arquivo para orquestrar o ambiente Docker.
‚îú‚îÄ‚îÄ Dockerfile                  # Arquivo de configura√ß√£o para a imagem Docker.
‚îî‚îÄ‚îÄ readme.md                   # Este arquivo.
```

## üõ†Ô∏è Setup e Execu√ß√£o do Ambiente

Para executar o projeto, utilizamos um ambiente Docker que j√° cont√©m o Jupyter e o PySpark configurados.

### Pr√©-requisitos
*   [Docker](httpss://docs.docker.com/get-docker/)
*   [Docker Compose](httpss://docs.docker.com/compose/install/)

### Passos para Execu√ß√£o

1.  **Clone o Reposit√≥rio**:
    ```bash
    git clone [<URL_DO_REPOSITORIO>](https://github.com/AleTavares/dataqualitySpark.git)
    cd dataqualitySpark
    ```

2.  **Inicie o Ambiente Docker**:
    No terminal, a partir da raiz do projeto, execute o comando abaixo. Ele ir√° construir a imagem Docker (se ainda n√£o existir) e iniciar o container do JupyterLab em background.
    ```bash
    docker compose up --build -d
    ```

3.  **Acesse o Jupyter**:
    Abra seu navegador e acesse o seguinte endere√ßo:
    ```
    http://localhost:8888/
    ```
    *   O token de acesso j√° est√° configurado no `docker-compose.yml` para simplificar.

4.  **Execute o Notebook**:
    *   Dentro do ambiente Jupyter, navegue at√© a pasta `notebooks`.
    *   Abra o arquivo `script.ipynb`.
    *   Execute as c√©lulas sequencialmente para acompanhar todo o processo de transforma√ß√£o dos dados.

5.  **Desligando o Ambiente**:
    Para parar e remover os containers, execute o seguinte comando no terminal:
    ```bash
    docker compose down
    ```

Com isso, o ambiente estar√° pronto para voc√™ explorar e aprender os conceitos fundamentais de qualidade de dados com PySpark!
