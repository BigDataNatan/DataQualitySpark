# Projeto de Data Quality com PySpark

Este repositÃ³rio foi criado com o objetivo de fornecer um guia prÃ¡tico e introdutÃ³rio ao Apache Spark, utilizando a linguagem Python (PySpark). O foco principal Ã© demonstrar um fluxo de trabalho essencial de engenharia de dados, cobrindo desde a ingestÃ£o de dados brutos atÃ© a aplicaÃ§Ã£o de tÃ©cnicas de qualidade de dados e o armazenamento em formatos otimizados.

## ðŸš€ Conceitos Abordados

O projeto, contido no notebook `notebooks/script.ipynb`, aborda os seguintes pilares de processamento de dados com Spark:

1.  **InicializaÃ§Ã£o e ConfiguraÃ§Ã£o**:
    *   CriaÃ§Ã£o de uma `SparkSession`, o ponto de entrada para qualquer aplicaÃ§Ã£o Spark.

2.  **IngestÃ£o de Dados**:
    *   Leitura de arquivos CSV, definindo um **schema explÃ­cito** para garantir a consistÃªncia dos tipos de dados e otimizar a performance de leitura.

3.  **Pipeline de Qualidade de Dados (Data Quality)**:
    *   **Limpeza (Cleaning)**: Tratamento de valores nulos (`null`) utilizando `coalesce` e `fillna`.
    *   **TransformaÃ§Ã£o (Transformation)**: PadronizaÃ§Ã£o de formatos (datas, strings) e conversÃ£o de tipos de dados (`cast`).
    *   **Enriquecimento (Enrichment)**: CriaÃ§Ã£o de novas colunas com base em regras de negÃ³cio (`when`/`otherwise`).
    *   **NormalizaÃ§Ã£o (Normalization)**: PadronizaÃ§Ã£o de valores categÃ³ricos para garantir consistÃªncia.
    *   **DesduplicaÃ§Ã£o (Deduplication)**: RemoÃ§Ã£o de registros duplicados para garantir a unicidade dos dados.

4.  **Armazenamento Otimizado**:
    *   Escrita dos dados tratados em formato **Parquet** e **ORC**, explicando os benefÃ­cios de formatos colunares.
    *   Uso de **particionamento** (`partitionBy`) para otimizar futuras consultas.

5.  **OtimizaÃ§Ã£o de Consultas**:
    *   DemonstraÃ§Ã£o prÃ¡tica do **Predicate Pushdown**, onde o Spark otimiza a leitura ao filtrar dados diretamente no nÃ­vel das partiÃ§Ãµes.
    *   DemonstraÃ§Ã£o da otimizaÃ§Ã£o de **SerDe (Projection Pushdown)**, onde apenas as colunas selecionadas em uma query sÃ£o lidas do disco.

## ðŸ“‚ Estrutura do Projeto

```
/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ logistica_raw.csv       # Dataset bruto para ser utilizado no exercicio.
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ geradorDataset.ipynb    # Notebook Jupter com o codigo para Gerar o arquivo CSV para o Exercicio.
â”‚   â”œâ”€â”€ script.ipynb            # Notebook Jupyter com todo o cÃ³digo PySpark.
â”‚   â””â”€â”€ vendas_brutas.csv       # Dataset bruto utilizado no exemplo.
â”œâ”€â”€ docker-compose.yml          # Arquivo para orquestrar o ambiente Docker.
â”œâ”€â”€ Dockerfile                  # Arquivo de configuraÃ§Ã£o para a imagem Docker.
â”œâ”€â”€ exercicio.md                # Arquivo com o Desafio para ser feito em aula
â””â”€â”€ readme.md                   # Este arquivo.
```

## ðŸ› ï¸ Setup e ExecuÃ§Ã£o do Ambiente

Para executar o projeto, utilizamos um ambiente Docker que jÃ¡ contÃ©m o Jupyter e o PySpark configurados.

### PrÃ©-requisitos
*   [Docker](httpss://docs.docker.com/get-docker/)
*   [Docker Compose](httpss://docs.docker.com/compose/install/)

### Passos para ExecuÃ§Ã£o

1.  **Clone o RepositÃ³rio**:
    ```bash
    git clone https://github.com/AleTavares/dataqualitySpark.git
    cd dataqualitySpark
    ```

2.  **Inicie o Ambiente Docker**:
    No terminal, a partir da raiz do projeto, execute o comando abaixo. Ele irÃ¡ construir a imagem Docker (se ainda nÃ£o existir) e iniciar o container do JupyterLab em background.
    ```bash
    docker compose up --build -d
    ```

3.  **Acesse o Jupyter**:
    Abra seu navegador e acesse o seguinte endereÃ§o:
    ```
    http://localhost:8888/
    ```
    *   O token de acesso jÃ¡ estÃ¡ configurado no `docker-compose.yml` para simplificar.

4.  **Execute o Notebook**:
    *   Dentro do ambiente Jupyter, navegue atÃ© a pasta `notebooks`.
    *   Abra o arquivo `script.ipynb`.
    *   Execute as cÃ©lulas sequencialmente para acompanhar todo o processo de transformaÃ§Ã£o dos dados.

> Com isso, o ambiente estarÃ¡ pronto para vocÃª explorar e aprender os conceitos fundamentais de qualidade de dados com PySpark!

> [!IMPORTANT]
> 5.  **Desligando o Ambiente**:
>    Para parar e remover os containers, execute o seguinte comando no terminal:
>    ```bash
>    docker compose down
>    ```