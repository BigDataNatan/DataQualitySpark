# üìù Desafio Final da Aula Pr√°tica: Refino de Dados de Log√≠stica

## üéØ Objetivo

Criar um *pipeline* PySpark completo para transformar dados brutos de log√≠stica na camada **Curated**, aplicando todas as 5 t√©cnicas de Data Quality, salvando no formato Parquet particionado e demonstrando a otimiza√ß√£o de I/O.

## 1. üìÇ Dados Brutos (`logistica_raw.csv`)

O arquivo de entrada (que ser√° gerado via c√≥digo na Tarefa A) cont√©m problemas de tipagem, nulos e duplicatas:

| ID_RASTREIO | DATA_ENVIO_RAW | CUSTO_FRETE_RAW | CIDADE_DESTINO | STATUS_ENTREGA |
| :---: | :---: | :---: | :---: | :---: |
| L9001 | 2024-05-10 | 15.50 | Sao Paulo | ENTREGUE |
| L9002 | 2024/05/11 | 22,00 | Rio | Em Tr√¢nsito |
| L9003 | 2024-05-12 | 100.00 | Salvador | null |
| L9004 | 2024-05-12 | null | BH | PENDENTE |
| L9004 | 2024-05-12 | null | BH | PENDENTE |

---

## 2. üöÄ Tarefa A: Ingest√£o e Data Quality (50%)

Utilize o c√≥digo de suporte fornecido para carregar os dados no `df_logistica_raw`. Em seguida, aplique as transforma√ß√µes em sequ√™ncia:

1.  **Pr√©-requisito:** Defina o `StructType` que trate **todas** as colunas como `StringType`.
2.  **Limpeza (Cleaning):**
    * Trate nulos na coluna `STATUS_ENTREGA`: Se for nulo, preencha com a *string* `"DESCONHECIDO"`.
    * Trate nulos na coluna `CUSTO_FRETE_RAW`: Se for nulo, preencha com a *string* `"0.00"`.
3.  **Transforma√ß√£o (Transformation):**
    * Crie uma coluna `DATA_ENVIO` convertendo `DATA_ENVIO_RAW` para o tipo `Date`.
    * Crie uma coluna `CUSTO_FRETE` convertendo `CUSTO_FRETE_RAW` para o tipo `Double` (remova v√≠rgulas antes do `cast`).
4.  **Enriquecimento (Enrichment):**
    * Crie uma coluna `FLAG_CARO`: Atribua `True` se `CUSTO_FRETE` for maior ou igual a `50.00`, sen√£o `False`.
5.  **Normaliza√ß√£o (Normalization):**
    * Converta a coluna `STATUS_ENTREGA` para **CAIXA ALTA**.
6.  **Desduplica√ß√£o (Deduplication):**
    * Use `ID_RASTREIO` como chave e remova as duplicatas. O resultado final deve ter **4 linhas**.

### **Resultado Esperado da Tarefa A:** Um DataFrame `df_logistica_curated` limpo, sem nulos, com tipos corretos, e 4 linhas.

---

## 3. üíæ Tarefa B: Armazenamento Otimizado (30%)

Salve o DataFrame final (`df_logistica_curated`) no Data Lake (diret√≥rio mapeado pelo Docker):

1.  **Escrita em Parquet:**
    * Salve o DataFrame em: `./data/curated/logistica_parquet`.
    * Use o modo `overwrite`.
    * **Particione o dataset** fisicamente pela coluna **`CIDADE_DESTINO`**.

---

## 4. üîé Tarefa C: Otimiza√ß√£o de I/O (20%)

Demonstre que o seu armazenamento est√° eficiente:

1.  **Predicate Pushdown:**
    * Crie uma vari√°vel `df_salvador`. Leia o arquivo Parquet **filtrando** apenas as entregas com `CIDADE_DESTINO` igual a `"Salvador"`.
    * Adicione um coment√°rio (`#`) explicando como o particionamento acelera esta consulta.

2.  **SerDe e Proje√ß√£o:**
    * Crie uma vari√°vel `df_leve`. Leia o arquivo Parquet **selecionando (projetando)** apenas as colunas `ID_RASTREIO` e `FLAG_CARO`.
    * Adicione um coment√°rio (`#`) explicando como o formato colunar e o SerDe otimizam a leitura (ignorando as colunas de data/custo).